{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Defacing Detector",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinhavlicek/OHBM-hackathons-on-using-DNNs/blob/master/Defacing_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCU4oOJ4kpjX",
        "colab_type": "text"
      },
      "source": [
        "# Keras for Neuroimaging\n",
        "\n",
        "Author: J. Andrew Doyle, Montreal Neurological Institute\n",
        "\n",
        "First presented at:\n",
        "\n",
        "[Montreal Artificial Intelligence & Neuroscience](http://www.crm.umontreal.ca/2018/MAIN2018/index_e.php): Hands-on Deep Learning with Keras workshop\n",
        "\n",
        "December 10, 2018\n",
        "\n",
        "Centre de Recherches Mathématiques, Université de Montréal\n",
        "\n",
        "Contact: [andrew.doyle@mcgill.ca](mailto:andrew.doyle@mcgill.ca)    Twitter: [@crocodoyle](https://twitter.com/crocodoyle)\n",
        "\n",
        "![Sponsor logos](https://amnesia.cbrain.mcgill.ca/deeplearning/img/sponsor_logos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCX_ZTWt2JrS",
        "colab_type": "text"
      },
      "source": [
        "# 1. Goal\n",
        "\n",
        "Properly **anonymizing data** is essential to be able to share data and collaborate. This notebook will demonstrate how to use [Keras](https://keras.io), a high-level deep learning library, to automatically **detect whether a T1w MRI scan has been defaced** or not.\n",
        "\n",
        "This notebook was inspired by the [Neuroinformatics & Google Summer of Code Defacing Detector project](https://summerofcode.withgoogle.com/archive/2018/projects/5287412167081984/). For that project, a more complex 2.5 D convolutional neural network was trained by Wazeer Zulfikar (co-mentored by Chris Gorgolewski). The trained neural network is currently being integrated into the [BIDS-Validator](https://github.com/bids-standard/bids-validator) that performs automatic checks that neuroimaging datasets are ready to share!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai9Hj-wdm6Qf",
        "colab_type": "text"
      },
      "source": [
        "The dataset prepared for this tutorial is a 2D, slimmed down version of [IXI](https://brain-development.org/ixi-dataset/), where participants are all healthy. It is the only publicly-available dataset I know of that includes faces of participants. This smaller version consists of sagittal slices of (1) the original MRI scans, and (2) \"defaced\" images, where [pydeface](https://github.com/poldracklab/pydeface) has been run to remove identifiable features from subjects' brain scans.\n",
        "\n",
        "![example of image original and defaced MRI slices](https://amnesia.cbrain.mcgill.ca/deeplearning/img/defacing_example.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N2nsqR33iq5",
        "colab_type": "text"
      },
      "source": [
        "Most machine learning techniques are probabilistic in nature. For this notebook, are are trying to estimate the probability that an image has been defaced. Let *D* represent a random variable where *D=0* denotes that defacing has *not* been done, and *D=1* means that defacing has been properly applied. *MRI* is a continuous-valued random field with 2D spatial structure that contains information about *D*. We wish to estimate the probability of *D* from our measurement *MRI*:\n",
        "\n",
        "> *P ( D | MRI )*\n",
        "\n",
        "More specifically, we will train **convolutional neural networks ** to compare the probabilities:\n",
        "\n",
        "\n",
        ">*P ( D=1 | MRI )* and *P ( D=0 | MRI )*\n",
        "\n",
        "Later on, *D* is refered to as `labels` and *MRI* as `images`\n",
        "\n",
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/morpheus.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rweNj89Ge9MI",
        "colab_type": "text"
      },
      "source": [
        "First, install Keras using `pip`, the most popular package manager for Python. We're also going to install [Scikit-Learn](https://scikit-learn.org/stable/) and use a few of its utilities, and download and unzip the dataset for this tutorial. Select the cell with your mouse or arrow keys, and hit Shift + Enter to run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-i9D8k2EEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras sklearn\n",
        "!wget https://amnesia.cbrain.mcgill.ca/deeplearning/ixi_slices.tar.gz --no-check-certificate\n",
        "!tar -xvf ixi_slices.tar.gz\n",
        "\n",
        "# data is split into two folders:\n",
        "# /slices/original/\n",
        "# /slices/defaced/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0BhKADbii0",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading Data\n",
        "The data in this example has two parts, the **images** (x) and a **label** (y) for whether it has been defaced or not. We can create the labels array based on which folder the scan is found in.\n",
        "\n",
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/files.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqH-BMWp5t27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob        # tool to match file patterns\n",
        "import numpy as np\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "original_images = glob('slices/original/*.jpg')\n",
        "defaced_images = glob('slices/defaced/*.jpg')\n",
        "\n",
        "# build labels array, starting as a list\n",
        "# you could probably do this more efficiently with list comprehensions\n",
        "# but I find this more readable\n",
        "labels = []\n",
        "\n",
        "for i in original_images:\n",
        "  labels.append(0)           # D=0 for original, undefaced\n",
        "\n",
        "for i in defaced_images:\n",
        "  labels.append(1)           # D=1 for defaced\n",
        "  \n",
        "labels = np.uint8(labels)\n",
        "image_filenames = original_images + defaced_images\n",
        "\n",
        "n_images = len(labels)\n",
        "n_channels = 3               # colour channels\n",
        "\n",
        "print('Created list with labels. There are', n_images, 'of them.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAkf8eg89lA3",
        "colab_type": "text"
      },
      "source": [
        "Now we have a list of filenames for *MRI* and a `numpy` array of integers for *D*, with matched indices.\n",
        "Next, we transform the *D* into \"one-hot encoding\", where integer categorical labels are represented as a probability vector:\n",
        "\n",
        "> *D=0* -> `[1, 0]`\n",
        "\n",
        "> *D=1* -> `[0, 1]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW8xY6eu_QN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "labels_one_hot = to_categorical(labels, num_classes=2)\n",
        "\n",
        "print('First label (original, one-hot encodings):', labels[0], labels_one_hot[0])\n",
        "print('Last label (original, one-hot encodings):', labels[-1], labels_one_hot[-1])\n",
        "\n",
        "print('Labels shape in one-hot encoding:', labels_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlNlU5d8_tCu",
        "colab_type": "text"
      },
      "source": [
        "Then we load the image data. It's currently stored as .jpg in `uint8` (0 - 255), and GPUs work with `float32`, so we convert to float and rescale between 0 -1. This linear rescaling is a fairly standard approach in deep learning, and we avoid whitening the data so that it has zero mean and unit standard deviation, like in most other machine learning methods.\n",
        "\n",
        "![Normalizing data](https://amnesia.cbrain.mcgill.ca/deeplearning/img/normalize.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxKO547ZqzfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import skimage   # image processing lib. OpenCV good too. ITK for the bold\n",
        "\n",
        "image_data = np.zeros((n_images, 224, 224, n_channels), dtype=np.float32)\n",
        "\n",
        "for i, img_filename in enumerate(image_filenames):\n",
        "  img = plt.imread(img_filename)\n",
        "  img = skimage.img_as_float(img)\n",
        "  img = img / np.max(img)\n",
        "  \n",
        "  image_data[i, :, :, :] = img\n",
        "\n",
        "  \n",
        "print('First image (not-defaced):')\n",
        "plt.imshow(image_data[0, :, :, :])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('Last image (defaced):')\n",
        "plt.imshow(image_data[-1, :, :, :])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMMvpFiYZf_I",
        "colab_type": "text"
      },
      "source": [
        "## Train / Test Split \n",
        "At this point we should probably split up our data into a real training and test set. The `sklearn` package has some [very nice functions](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for splitting data into training(/validation)/testing sets. We're going to use the most basic one, `train_test_split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itd-h9N_ndB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "image_data_train, image_data_test, labels_train, labels_test = train_test_split(image_data, labels_one_hot, test_size=0.1, random_state=42)\n",
        "\n",
        "# when in doubt, print all the shapes\n",
        "# print all the shapes when not in doubt too\n",
        "print('Training images shape:', image_data_train.shape)\n",
        "print('Training labels shape:', labels_train.shape)\n",
        "\n",
        "print('Test images shape:', image_data_test.shape)\n",
        "print('Test labels shape:', labels_test.shape)\n",
        "\n",
        "# check that the distribution of faced/defaced data is similar\n",
        "label_integers_train = np.argmax(labels_train, axis=-1)\n",
        "label_integers_test = np.argmax(labels_test, axis=-1)\n",
        "\n",
        "# make a histogram\n",
        "f, axes = plt.subplots(1, 2, sharey=True)\n",
        "axes[0].hist(label_integers_train)\n",
        "axes[1].hist(label_integers_test)\n",
        "\n",
        "axes[0].set_xlabel('Train', fontsize=20)\n",
        "axes[1].set_xlabel('Test', fontsize=20)\n",
        "\n",
        "for ax in axes:\n",
        "  ax.set_xticks([0, 1])\n",
        "  ax.set_xticklabels(['Undefaced', 'Defaced'], fontsize=16)\n",
        "  \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiLdxn3XhP0e",
        "colab_type": "text"
      },
      "source": [
        "# 3. Sequential API\n",
        "The [Sequential API](https://keras.io/getting-started/sequential-model-guide/) is the easiest to get up and running with. You just declare a Sequential object and progressively add layers to it. When you're done, `compile` it with an optimizer and loss function, then fit it with some data. In this section we're going to look at a few different architectures defined with the Sequential API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahve-fsEc2h2",
        "colab_type": "text"
      },
      "source": [
        "## Fully-Connected\n",
        "In fully-connected (or densely-connected, or multi-layer perceptrons (MLPs)) networks, inputs to each layer are connected to every artificial neuron of the previous layer. Each neuron can be thought of as modeling a joint relationship between every MRI input voxel or every hidden layer feature. This is very powerful, but very inefficient.\n",
        "\n",
        "![Fully-connected](https://amnesia.cbrain.mcgill.ca/deeplearning/img/fc.png)\n",
        "\n",
        "The following example is just a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier, that takes every pixel as input and has a single layer with a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation, which is just a normalized [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function so that it sums across the classes to 1 and we can call it a probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-CGkOZW574S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "# clear the GPU memory in case it's loaded up from a previous experiment\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "dense_model = Sequential()\n",
        "\n",
        "dense_model.add(Flatten(input_shape=(224, 224, 3)))     # first layers of Sequential models need this input_shape argument\n",
        "dense_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "dense_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "dense_model.summary()      # prints out a summary of the computational graph\n",
        "\n",
        "dense_model.fit(image_data_train, labels_train, epochs=10, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJKKoYgSh9dH",
        "colab_type": "text"
      },
      "source": [
        "This doesnt work very well, and doesn't appear to be training at all.\n",
        "\n",
        "![we need to go deeper](https://i.kym-cdn.com/photos/images/newsfeed/000/531/557/a88.jpg)\n",
        "\n",
        "Let's add another couple of layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjg1hxKGhSIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "dense_model = Sequential()\n",
        "\n",
        "dense_model.add(Flatten(input_shape=(224, 224, 3)))\n",
        "dense_model.add(Dense(64))\n",
        "dense_model.add(Dense(64))\n",
        "dense_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "dense_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "dense_model.summary()\n",
        "\n",
        "# can use validation_split argument to automatically create a validation set\n",
        "dense_model.fit(image_data_train, labels_train, batch_size=64, epochs=10, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCR0wAQJnmOr",
        "colab_type": "text"
      },
      "source": [
        "OK maybe going deeper isn't always the solution.\n",
        "\n",
        "![The Number of Parameters is Too Damn High](https://amnesia.cbrain.mcgill.ca/deeplearning/img/too_damn_high.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XNbsXAflswy",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "Let's define our first convolutional neural network. We're going to stack a few rectifying (reLU) [Conv2D](https://keras.io/layers/convolutional/#conv2d) layers and [MaxPool2D](https://keras.io/layers/pooling/#maxpooling2d) layers to learn some feature extractor, then train a 2-layer [Dense](https://keras.io/layers/core/#dense) classifier for prediction. The single [Batch Normalization](https://keras.io/layers/normalization/) layer really helps here, too.\n",
        "\n",
        "![Non Linearities](https://amnesia.cbrain.mcgill.ca/deeplearning/img/non-linearities.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGC3Us_SejqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization\n",
        "from keras.layers import Activation\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "my_model = Sequential()\n",
        "\n",
        "my_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "my_model.add(BatchNormalization())\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(64, kernel_size=3))\n",
        "my_model.add(Activation('relu'))           # you can also add activations independently\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "# at this point in the model, the spatial arrangement of filter outputs\n",
        "# is still present and we need to predict a vector of size (2), so we Flatten\n",
        "\n",
        "my_model.add(Flatten())\n",
        "\n",
        "# now we have extracted features from the data, and we need to classify it.\n",
        "# we can think of this last part as a multi-layer perceptron\n",
        "# most of the parameters in the model are here\n",
        "\n",
        "my_model.add(Dense(64, activation='relu'))\n",
        "my_model.add(Dense(64, activation='relu'))\n",
        "my_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "my_model.summary()\n",
        "\n",
        "my_model.fit(image_data_train, labels_train, batch_size=64, epochs=5, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXrIhDGUnKly",
        "colab_type": "text"
      },
      "source": [
        "Keras Models have a bunch of different methods other than the magical `.fit`. If you want to test whether your model works on a held-out test set, you can use [.evaluate ](https://)to return the loss, and any [metrics](https://keras.io/metrics/) that you compiled the model with. `.predict` returns probabilities of the classes you trained your model to predict, which you can turn into categorical labels (0 for undefaced, 1 for defaced) with `np.argmax()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0-9Ng_qacXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = my_model.evaluate(image_data_test, labels_test)\n",
        "print(my_model.metrics)\n",
        "print('Loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "prediction_probabilities = my_model.predict(image_data_test)\n",
        "predictions = np.argmax(prediction_probabilities, axis=-1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = np.argmax(labels_test, axis=-1)\n",
        "\n",
        "def plot_confusion(y_true, y_pred):\n",
        "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  print('Confusion matrix shape:', conf_matrix.shape)\n",
        "  \n",
        "  plt.imshow(conf_matrix)\n",
        "  plt.xticks([0, 1], ['Undefaced', 'Defaced'], fontsize=16)\n",
        "  plt.yticks([0, 1], ['Undefaced', 'Defaced'], fontsize=16)\n",
        "  plt.ylabel('Truth', fontsize=20)\n",
        "  plt.xlabel('Prediction', fontsize=20)\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "plot_confusion(y_true, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhhAP3Wgm-jv",
        "colab_type": "text"
      },
      "source": [
        "Hurray, it appears to work on our test set too!\n",
        "\n",
        "![Non Linearities](https://amnesia.cbrain.mcgill.ca/deeplearning/img/gandalf.png)\n",
        "\n",
        "What happens if we flip the images though? It is conceivable that we'd want to use this on images from other datasets, which might not have the same orientations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl3E5-hLcBAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flipped_test_images = image_data_test[:, :, ::-1, :]\n",
        "\n",
        "plt.imshow(flipped_test_images[4, ...])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "score = my_model.evaluate(flipped_test_images, labels_test)\n",
        "print('Accuracy:', score[1])\n",
        "\n",
        "test_probs = my_model.predict(flipped_test_images)\n",
        "test_predictions = np.argmax(test_probs, axis=-1)\n",
        "\n",
        "plot_confusion(y_true, test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENV5cwmto4Xl",
        "colab_type": "text"
      },
      "source": [
        "FAIL\n",
        "\n",
        "![one does not simply train a network that will generalize across datasets](https://amnesia.cbrain.mcgill.ca/deeplearning/img/one_does_not.png)\n",
        "\n",
        "To start to address this, we can use data augmentation, to cover more of the distribution of MRI than we actually have access to. For computer vision, this usually involves random flipping and affine transformations. For neuroimaging data it is not always as clear how you would do this. Keras has [built-in data generators for common data types](https://keras.io/preprocessing/image/). We can use the image one to randomly apply transformations to our dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju7Jbu-edUB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_img_gen = ImageDataGenerator(\n",
        "#     rotation_range=10,\n",
        "#     width_shift_range=0.05,\n",
        "#     height_shift_range=0.05,\n",
        "#     shear_range=0.02,\n",
        "#     zoom_range=0.02,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_generator = train_img_gen.flow_from_directory(\n",
        "    'slices',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=['original', 'defaced']\n",
        ")\n",
        "\n",
        "my_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=n_images // batch_size,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# this is cheating now (\"double-dipping\") because we've used our test set\n",
        "# as part of the training set, because the generator is itself loading all\n",
        "# the images from the file system\n",
        "scores = my_model.evaluate(flipped_test_images, labels_test)\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "class_probs = my_model.predict(flipped_test_images)\n",
        "\n",
        "y_true = np.argmax(labels_test, axis=-1)\n",
        "y_pred = np.argmax(class_probs, axis=-1)\n",
        "\n",
        "plot_confusion(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJvB6OpquHtH",
        "colab_type": "text"
      },
      "source": [
        "These generators are also very useful because in general, your datasets will probably not fit into RAM, let alone GPU memory, so you can't pass your entire training dataset into `.fit`. Here's an example of how you could write your own generator to load data on-the-fly, and Keras will do the multi-threading for you.\n",
        "\n",
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/gpus.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9jjxj6mvsgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_generator(image_data, labels, batch_size=32):\n",
        "      n_images = image_data.shape[0]\n",
        "    \n",
        "      x = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "      y = np.zeros((batch_size, 2), dtype=np.int8)\n",
        "      \n",
        "      indices = np.asarray(range(n_images))\n",
        "      \n",
        "      # this loop is for one epoch\n",
        "      while True:\n",
        "          np.random.shuffle(indices)\n",
        "\n",
        "          batch_idx = 0\n",
        "          for i, image_index in enumerate(indices):\n",
        "              batch_idx = i % batch_size\n",
        "              image = image_data[image_index, ...]\n",
        "              \n",
        "              # randomly flip image              \n",
        "              if np.random.rand() > 0.5:\n",
        "                image = np.flipud(image)\n",
        "              if np.random.rand() > 0.5:\n",
        "                image = np.fliplr(image)\n",
        "            \n",
        "              x[batch_idx, ...] = image\n",
        "              y[batch_idx] = labels[image_index, :]\n",
        "              \n",
        "              if i % batch_size == 0 and i != 0:\n",
        "                yield (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twjN3wjSsVMH",
        "colab_type": "text"
      },
      "source": [
        "If working with MRI images, you could use [nibabel](http://nipy.org/nibabel/) to load your data in a generator loop like this, or [mne-python](https://martinos.org/mne/stable/index.html) if using EEG/MEG data, or [OpenSlide](https://openslide.org/) for histology data.\n",
        "\n",
        "I usually load the entirety of my dataset and dump it into a gigantic HDF5 file. HDF5 is a nice format, because it lets you load only parts of files into memory. This makes it far more efficient to sample if you don't need entire data examples, which are typically huge in neuroimaging. [h5py](https://www.h5py.org/) is one library that makes manipulating HDF5 files easy.\n",
        "\n",
        "Now we can train with our custom generator using the `.fit_generator` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhfXX8N4tVhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model.fit_generator(\n",
        "    my_generator(image_data_train, labels_train),\n",
        "    steps_per_epoch=n_images // batch_size,\n",
        "    epochs=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6nj-jJvivKe",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/over-9000.png)\n",
        "\n",
        "Here's a recipe that usually works pretty well for image/volumetric data:\n",
        "\n",
        "1.   Stack: (1) ReLU 3x3(x3) Convolutional, (2) Batch Normalization, (3) Dropout and (4) Max Pooling layers.\n",
        "2.   Repeat four or five times\n",
        "3.   Flatten the output\n",
        "4.   Add two fully-connected ReLU layers followed by Dropout layers\n",
        "5.   Final softmax classification layer with the number of units equal to the number of classes\n",
        "\n",
        "Train with [Adam](https://keras.io/optimizers/) optimizer, with the learning rate set an order of magnitude less than the default. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctGs5SV_ZmT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPool2D, BatchNormalization\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "my_model = Sequential()\n",
        "\n",
        "my_model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(224, 224, 3)))\n",
        "my_model.add(BatchNormalization())\n",
        "# my_model.add(Dropout(0.5))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "# my_model.add(BatchNormalization())\n",
        "# my_model.add(Dropout(0.5))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "# my_model.add(BatchNormalization())\n",
        "# my_model.add(Dropout(0.5))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "# my_model.add(BatchNormalization())\n",
        "# my_model.add(Dropout(0.5))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "# my_model.add(BatchNormalization())\n",
        "# my_model.add(Dropout(0.5))\n",
        "my_model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "my_model.add(Flatten())\n",
        "\n",
        "# now we have extracted features from the data, and we need to classify it\n",
        "# we can think of this last part as a multi-layer perceptron\n",
        "# most of the trainable parameters in the model are here\n",
        "\n",
        "my_model.add(Dense(64, activation='relu'))\n",
        "my_model.add(Dropout(0.5))\n",
        "my_model.add(Dense(64, activation='relu'))\n",
        "my_model.add(Dropout(0.5))\n",
        "my_model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# the Adam optimizer usually works well for convolutional neural networks.\n",
        "# the learning rate usually needs to be tuned; usually an order of magnitude\n",
        "# less than the default works well.\n",
        "#\n",
        "# the 'decay' parameter is an L2 regularizer on the value of the parameters,\n",
        "# which constrains them from growing and growing and growing out of control\n",
        "# until you end up with numerical issues\n",
        "optimizer  = Adam(lr=0.0002, decay=1e-5)\n",
        "\n",
        "my_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-VwVg23i0E8",
        "colab_type": "text"
      },
      "source": [
        "[Callbacks](https://keras.io/callbacks/) allow you to call functions every epoch. There are some very useful built-in ones, like the `ModelCheckpoint`, which will save the best model found during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6taMPCOaiWPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "best_model_callback = ModelCheckpoint(filepath='/best_model.pkl', monitor='val_loss', save_best_only=True)\n",
        "my_model.fit(image_data_train, labels_train, batch_size=32, epochs=10, callbacks=[best_model_callback], validation_split=0.1)\n",
        "\n",
        "# load the best model found during training\n",
        "from keras.models import load_model\n",
        "my_model = load_model('/best_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Ra13xFgaP3",
        "colab_type": "text"
      },
      "source": [
        "# 4. Functional API\n",
        "\n",
        "The [functional API ](https://keras.io/getting-started/functional-api-guide/) is a little bit more complicated, but far more flexible. It allows you to do things like have multiple inputs/outputs to a network and split/merge branches. You might want to do this if you wanted to use both imaging data and clinical metadata as inputs, or jointly predict several different clinical measures from imaging data.\n",
        "\n",
        "We're not going to do this here, but it's also necessary for using pre-trained networks, which we'll get to.\n",
        "\n",
        "Basically layers (or groups of layers) are now functions, and then to specify that you want to pass the output of a layer `L1` into layer `L2`, you pass `L1` as you would a variable into a normal Python function:\n",
        "\n",
        "`L1_output = L2(L1)`\n",
        "\n",
        "You then chain these all together to produce the entire deep network, and tell a `Model` object what the input(s) and output is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOdInJG1TLx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPool2D, BatchNormalization\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "# an Input layer is required by the functional API\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# layers are treated as functions, and are \"called\" with inputs, which are the\n",
        "# outputs of previous layers\n",
        "conv1 = Conv2D(32, kernel_size=3, activation='relu')(inputs)\n",
        "norm1 = BatchNormalization()(conv1)\n",
        "drop1 = Dropout(0.5)(norm1)\n",
        "pool1 = MaxPool2D(pool_size=2)(drop1)\n",
        "\n",
        "conv2 = Conv2D(32, kernel_size=3, activation='relu')(pool1)\n",
        "norm2 = BatchNormalization()(conv2)\n",
        "drop2 = Dropout(0.5)(norm2)\n",
        "pool2 = MaxPool2D(pool_size=2)(drop2)\n",
        "\n",
        "conv3 = Conv2D(32, kernel_size=3, activation='relu')(pool2)\n",
        "norm3 = BatchNormalization()(conv3)\n",
        "drop3 = Dropout(0.5)(norm3)\n",
        "pool3 = MaxPool2D(pool_size=2)(drop3)\n",
        "\n",
        "conv4 = Conv2D(32, kernel_size=3, activation='relu')(pool3)\n",
        "norm4 = BatchNormalization()(conv4)\n",
        "drop4 = Dropout(0.5)(norm4)\n",
        "pool4 = MaxPool2D(pool_size=2)(drop4)\n",
        "\n",
        "conv5 = Conv2D(32, kernel_size=3, activation='relu')(pool4)\n",
        "norm5 = BatchNormalization()(conv5)\n",
        "drop5 = Dropout(0.5)(norm5)\n",
        "pool5 = MaxPool2D(pool_size=2)(drop5)\n",
        "\n",
        "flat = Flatten()(pool5)\n",
        "\n",
        "fc1 = Dense(64, activation='relu')(flat)\n",
        "drop_fc1 = Dropout(0.5)(fc1)\n",
        "fc2 = Dense(64, activation='relu')(drop_fc1)\n",
        "drop_fc2 = Dropout(0.5)(fc2)\n",
        "\n",
        "out = Dense(n_classes, activation='softmax')(drop_fc2)\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=0.002, decay=1e-5)\n",
        "\n",
        "my_functional_model = Model(inputs=[inputs], outputs=[out])\n",
        "\n",
        "my_functional_model.summary()\n",
        "\n",
        "my_functional_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "my_functional_model.fit(image_data_train, labels_train, batch_size=64, epochs=10, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRLeYMmppfdZ",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5DSbxzopcon",
        "colab_type": "text"
      },
      "source": [
        "Many famous models are available in Keras to be re-used on other problems as [Applications](https://keras.io/applications/). They are mostly available trained on [ImageNet](http://www.image-net.org/), a computer vision competition database that contains over 14 million images that are hierarchically categorized according to the [WordNet](https://wordnet.princeton.edu/) linguistic ontology. The most common competition is ImageNet-1000, where the images are categorized in 1000 different categories.\n",
        "\n",
        "But we don't have 1000 different categories, we have 2, and they don't correspond to the cats or dogs that make up the ImageNet categories, so we have to make some modifications. Here's the original VGG16 network that we're going to use:\n",
        "\n",
        "![VGG16 Architecture](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
        "\n",
        "Source: [Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014)](https://arxiv.org/abs/1409.1556)\n",
        "\n",
        "The reason that this tutorial uses 224x224 images with 3 colour channels is so that this example works. Also regular images are far easier to display with 3 channels; there are no 1-channel memes. Normally, single modalities of structural MRI scans have only 1 channel: a single voxel intensity value. This can be very confusing, as you might have to artificially expand your own data's dimensionality to include this channel dimension. Most structural MRI images are 3D `(height, width, depth)`, but you would have to train a network with input batches of size `(batch_size, height, width, depth, channels)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ6bkAGn2iA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# here we specify that we don't want to include the top part of the network\n",
        "# that contains the final layers that predict the ImageNet1000 categories.\n",
        "# Keras also has an optional pooling argument for what to do if the top is not\n",
        "# included. Here, we use Global Average Pooling, which averages the activations\n",
        "# of each convolutional filter in the previous layer.\n",
        "# This is like Flatten(), but you lose information.\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(224, 224, 3))\n",
        "\n",
        "# turn off training for the feature extraction portion\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# add a couple of layers using the functional API \n",
        "dense1 = Dense(128, activation='relu')(pretrained_model.output)\n",
        "dense2 = Dense(128, activation='relu')(dense1)\n",
        "\n",
        "defacing_prediction = Dense(2, activation='softmax')(dense2)\n",
        "\n",
        "# the 'decay' parameter is an L2 regularizer on the value of the parameters,\n",
        "# which constrains them from growing and growing and growing out of control\n",
        "# until you end up with numerical issues\n",
        "optimizer  = Adam(lr=0.0002, decay=1e-5)\n",
        "\n",
        "transfer_model = Model(inputs=pretrained_model.input, output=defacing_prediction)\n",
        "\n",
        "transfer_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "transfer_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocetjdUcnOnN",
        "colab_type": "text"
      },
      "source": [
        "There's a hidden callback called `History` that is called automatically, and tracks the loss function and whatever `metrics` you tell the model to compute. When you call any train method, it returns the history, which can be used to visualize the training dynamics. This is super useful to decide if you need to tweak your optimizer's learning rate, if your model is overfitting (when the validation accuracy starts to get worse but training accuracy improves)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3csJZQPhx06e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = transfer_model.fit(\n",
        "    x=image_data_train,\n",
        "    y=labels_train,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "plt.plot(hist.history['acc'], label='train')\n",
        "plt.plot(hist.history['val_acc'], label='val')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baPSvQQdkDHc",
        "colab_type": "text"
      },
      "source": [
        "# 5. Autoencoders\n",
        "\n",
        "In this little example, we're not going to try to predict `D`, we're going to train a little network to compress our data and learn an encoding that will reconstruct the input. This is a little bit like Principle Component Analysis, except much more powerful. A single-layer autoencoder with linear activation can learn the same dimensionality reduction as PCA, but you can also use more units, less units, add noise to the inputs, and of course go much deeper.\n",
        "\n",
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/brace-yourselves.png)\n",
        "\n",
        "Here's an example of a convolutional autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ve98wB4kF4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "input_img = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D(pool_size=2, padding='same')(x)\n",
        "\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D(size=2)(x)\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = UpSampling2D(size=2)(x)\n",
        "x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = UpSampling2D(size=2)(x)\n",
        "decoded = Conv2D(3, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.fit(image_data, image_data,\n",
        "                epochs=10,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_split=0.1\n",
        "               )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrzzBp4tk6FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = autoencoder.predict(image_data[0:1, ...])\n",
        "\n",
        "f, axes = plt.subplots(1, 2)\n",
        "axes[0].imshow(image_data[0, ...])\n",
        "axes[1].imshow(prediction[0, ...])\n",
        "axes[0].grid('off')\n",
        "axes[1].grid('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdurNEpO5atf",
        "colab_type": "text"
      },
      "source": [
        "It doesn't really work too well and I didn't spend very much time on this example. Increasing the accuracy is left as an exercise to the reader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyeLFdE4LfbM",
        "colab_type": "text"
      },
      "source": [
        "# 6. Interpretability\n",
        "What are these deep networks even learning? This section is going to explore a few different ways to begin to understand the models, and why they make predictions.\n",
        "\n",
        "![Everyone gets a heatmap](https://amnesia.cbrain.mcgill.ca/deeplearning/img/and-then.png)\n",
        "\n",
        " In the first section, we're going to use the `keras-vis` package, which creates visualizations for the purposes of interpreting predictions from neural networks. There's another package, `innvestigate`, that implements LRP, but it's not as well documented. We're going to go through a few techniques here:\n",
        "\n",
        "1.   Filter Activations\n",
        "2.   Class Appearance Models\n",
        "3.   Activation Maximization\n",
        "4.   LIME\n",
        "5.   Layer-wise Relevance Propagation\n",
        "\n",
        "This section assumes that `my_model` is a trained model. OK let's get started interpreting what it learned. Let's start by visualizing the activations of filters deeper into our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLawBom-_QYO",
        "colab_type": "text"
      },
      "source": [
        "## Filter Activations\n",
        "\n",
        "Here's how you can show, for some input, the activation of some filter anywhere in the network. There are a lot of these filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPaTXdb1Z71X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def show_activation(input_image, layer_name, filter_idx):\n",
        "    \n",
        "    # create a structure to more easily access layers by name\n",
        "    layer_lookup = dict([(layer.name, layer) for layer in my_model.layers])\n",
        "\n",
        "    layer_output = layer_lookup[layer_name].output\n",
        "\n",
        "    # create a function that stops the forward pass at the layer we want\n",
        "    partial_net = K.function([my_model.input], [layer_output]\n",
        "\n",
        "    # add a batch dimension to the image\n",
        "    input_image = input_image[np.newaxis, ...]\n",
        "    \n",
        "    activation = partial_net([input_image])[0] \n",
        "    activation = activation[0, :, :, filter_idx]\n",
        "    \n",
        "    f, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(input_image[0, ...])\n",
        "    axes[1].imshow(activation)\n",
        "    axes[0].grid('off')\n",
        "    axes[1].grid('off')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-qWjoqcaoCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the names of the layers in our model\n",
        "for layer in my_model.layers:\n",
        "  print(layer.name)\n",
        "  \n",
        "for layer in my_model.layers:\n",
        "  if 'conv' in layer.name:\n",
        "    show_activation(image_data_train[0, ...], layer.name, 0)\n",
        "#     show_activation(image_data_train[0, ...], layer.name, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_Bk7ROfv-3",
        "colab_type": "text"
      },
      "source": [
        "This gets uninterpretable pretty quickly, and the deeper you go, the less clear it is what is happening. Let's try some more complicated methods. First, install `keras-vis`, a great package that implements saliency, Grad-CAM, and Class Appearance Models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq2ca_-gs4qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/raghakot/keras-vis.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_68n0OViBz",
        "colab_type": "text"
      },
      "source": [
        "[TensorFlow](https://www.tensorflow.org/) is the default backend of Keras, and TensorFlow's computational graph is static after you've compiled it. This is a problem for many interpretability methods because they involve propagating the gradient back to the input, which is not possible with a softmax layer at the end, because the softmax gets rid of the gradient by squashing the outputs so that they sum to 1. This means that each output depends on all of the weights, which is bad for gradients!\n",
        "\n",
        "The first step is to use a `keras-vis` utility to remove the last layer (softmax), and replace it with a linear one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcDK_H3tetc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "if my_model.layers[-1].activation != activations.linear:\n",
        "  print('Modifying computational graph...')\n",
        "  my_model.layers[-1].activation = activations.linear\n",
        "  my_model = utils.apply_modifications(my_model)\n",
        "else:\n",
        "  print('Last layer is alreary linear')\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDKa2WMGXWw_",
        "colab_type": "text"
      },
      "source": [
        "## Class Appearance Model\n",
        "\n",
        "We're going to generate the input that the model thinks is the most representative single input for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhSg2cFsvVwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from vis.visualization import visualize_activation\n",
        "\n",
        "layer_idx = -1\n",
        "\n",
        "print('Computing faced activation...')\n",
        "faced_activation = visualize_activation(my_model, layer_idx, filter_indices=0)\n",
        "print('Computing defaced activation...')\n",
        "defaced_activation = visualize_activation(my_model, layer_idx, filter_indices=1)\n",
        "\n",
        "# plot results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(faced_activation)\n",
        "axes[1].imshow(defaced_activation)\n",
        "axes[0].grid('off')\n",
        "axes[1].grid('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nZTt3BNgVGW",
        "colab_type": "text"
      },
      "source": [
        "This is.... not obvious. There are other ways to do this, but it's never obvious how much information about the model you can extract from them.\n",
        "\n",
        "You can use algorithms like DeepDream to do \"gradient ascent\" at the output or at intermediate layers to do the same kind of thing. More information:\n",
        "*   [Deep Dream notebook](https://colab.research.google.com/drive/1DWcrN9WXni58MbddvlShX0wF_oeo8W_0)\n",
        "*   [Visualizing Features with Keras](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
        "*   [Feature Visualization Distill.pub article](https://distill.pub/2017/feature-visualization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-AIxlUgB7Fd",
        "colab_type": "text"
      },
      "source": [
        "# Individual Predictions\n",
        "\n",
        "Here are some ways you can explain individual predictions.\n",
        "\n",
        "![Everyone gets a heatmap](https://amnesia.cbrain.mcgill.ca/deeplearning/img/you-get-a-heatmap.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VimXChTXh2v",
        "colab_type": "text"
      },
      "source": [
        "## Saliency\n",
        "Saliency is all about the Jacobian, and a way to do sensitivity analysis. It uses the derivative of the output prediction with respect to each input pixel.\n",
        "\n",
        "You can also do this for hidden layers! The second parameter in `visualize_saliency` chooses the index of the layer.\n",
        "\n",
        "Paper: [https://arxiv.org/abs/1312.6034](https://arxiv.org/abs/1312.6034)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsY8ZehYWO-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "sample_idx = 1\n",
        "class_idx = int(labels[sample_idx])\n",
        "\n",
        "test_image = image_data[sample_idx, ...]\n",
        "display_image = np.uint8(test_image * 255)\n",
        "\n",
        "print('Computing visual saliency...')\n",
        "saliency_gradients = visualize_saliency(my_model, -1, filter_indices=class_idx, seed_input=test_image)\n",
        "saliency_heatmap = np.uint8(cm.jet(saliency_gradients)[..., :3] * 255)\n",
        "saliency_overlay = overlay(saliency_heatmap, display_image)\n",
        "\n",
        "# plot output\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 8))\n",
        "axes[0].imshow(test_image)\n",
        "axes[1].imshow(saliency_gradients, cmap='jet')\n",
        "axes[2].imshow(saliency_overlay)\n",
        "\n",
        "for ax in axes:\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuR_u84Is37N",
        "colab_type": "text"
      },
      "source": [
        "Lots of saliency in the face area! Looks good! How about for the other class?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdt7edAZFltq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "sample_idx = 1\n",
        "class_idx = labels[sample_idx]\n",
        "other_class_idx = np.abs(1 - class_idx)  # saliency for other class\n",
        "\n",
        "test_image = image_data[sample_idx, ...]\n",
        "display_image = np.uint8(test_image * 255)\n",
        "\n",
        "print('Computing visual saliency...')\n",
        "saliency_gradients = visualize_saliency(my_model, -1, filter_indices=other_class_idx, seed_input=test_image)\n",
        "saliency_heatmap = np.uint8(cm.jet(saliency_gradients)[..., :3] * 255)\n",
        "saliency_overlay = overlay(saliency_heatmap, display_image)\n",
        "\n",
        "# plot output\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 8))\n",
        "axes[0].imshow(test_image)\n",
        "axes[1].imshow(saliency_gradients, cmap='jet')\n",
        "axes[2].imshow(saliency_overlay)\n",
        "\n",
        "for ax in axes:\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADeaSOZWGCq_",
        "colab_type": "text"
      },
      "source": [
        "![This is fine](https://amnesia.cbrain.mcgill.ca/deeplearning/img/this-is-fine.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuy1weiHXoSd",
        "colab_type": "text"
      },
      "source": [
        "## Gradient-weighted Class Activation Map\n",
        "\n",
        "This example computes the Grad-CAM gradient-weighted class activation map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhFvrFFPX-Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_cam, overlay\n",
        "\n",
        "sample_idx = 1\n",
        "class_idx = int(labels[sample_idx])\n",
        "\n",
        "test_image = image_data[sample_idx, ...]\n",
        "display_image = np.uint8(test_image * 255)\n",
        "\n",
        "print('Computing class activation...')\n",
        "cam_gradients = visualize_cam(my_model, -1, filter_indices=class_idx, seed_input=test_image, backprop_modifier=None)\n",
        "cam_heatmap = np.uint8(cm.jet(cam_gradients)[..., :3] * 255)\n",
        "cam_overlay = overlay(cam_heatmap, display_image)\n",
        "\n",
        "print('Grad-CAM for class:', class_idx)\n",
        "# plot output\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 8))\n",
        "axes[0].imshow(test_image)\n",
        "axes[1].imshow(cam_gradients, cmap='jet')\n",
        "axes[2].imshow(cam_overlay)\n",
        "for ax in axes:\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yn11r-pHaV1",
        "colab_type": "text"
      },
      "source": [
        "Let's try multiplying the Grad-CAM with Guided Backprop. This is just the `backprop_modifier` parameter to `visualize_cam`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7OWnQdVr_qF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_cam, overlay\n",
        "\n",
        "sample_idx = 1\n",
        "class_idx = int(labels[sample_idx])\n",
        "\n",
        "test_image = image_data[sample_idx, ...]\n",
        "display_image = np.uint8(test_image * 255)\n",
        "\n",
        "print('Computing class activation...')\n",
        "cam_guided_gradients = visualize_cam(my_model, -1, filter_indices=class_idx, seed_input=test_image, backprop_modifier='guided')\n",
        "cam_heatmap = np.uint8(cm.jet(cam_guided_gradients)[..., :3] * 255)\n",
        "cam_overlay = overlay(cam_heatmap, display_image)\n",
        "\n",
        "# plot output\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 8))\n",
        "axes[0].imshow(test_image)\n",
        "axes[1].imshow(cam_guided_gradients, cmap='jet')\n",
        "axes[2].imshow(cam_overlay)\n",
        "for ax in axes:\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYDfiphH0TB",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://amnesia.cbrain.mcgill.ca/deeplearning/img/not-sure.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c8GA2sigWZB",
        "colab_type": "text"
      },
      "source": [
        "## LIME\n",
        "This method does some feature selection and then starts with the assumption that even highly non-linear classifiers, around the space of the input, can be approximated as linear. This means that small perturbations of the input shouldn't result in huge changes in the output.\n",
        "\n",
        "It relies on feature selection (and selecting a method to select features, which in and of itself is a challenge), and is not always 100% stable, but here it is!\n",
        "\n",
        "GitHub repository with high-level explanation of how it works: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)\n",
        "\n",
        "Paper: [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwLzcp3yfljE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvK89rB2hTJS",
        "colab_type": "text"
      },
      "source": [
        "First, let's take our trained classifier and try to predict whether there's a face in some images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zvtvBfg5Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_idx = 1\n",
        "\n",
        "# when using .predict, input data shape has to be: (batch_size, dim1, ..., dimN, n_channels)\n",
        "# this can be super confusing, keeping track of where the batch dimension and where the channel dimension is\n",
        "\n",
        "# if we take a slice of size 1 as opposed to 0, we can preserve that numpy dimension\n",
        "test_image = image_data[sample_idx:sample_idx+1, ...]\n",
        "print('Batch shape:', test_image.shape)\n",
        "\n",
        "predictions = my_model.predict(test_image)\n",
        "print('Model predictions:', predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVvFJ9rIipF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lime\n",
        "from lime import lime_image\n",
        "\n",
        "sample_idx = 1\n",
        "\n",
        "test_image = image_data[sample_idx, ...]\n",
        "class_idx = labels[sample_idx]\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(test_image, my_model.predict, top_labels=2, hide_color=0, num_samples=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDwVcE7UoQTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_lime_explanation(original_image, explanation_image, explanation_mask):\n",
        "  from skimage.segmentation import mark_boundaries\n",
        "  \n",
        "  fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "  axes[0].imshow(original_image)\n",
        "  axes[1].imshow(mark_boundaries(explanation_image, explanation_mask))\n",
        "  for ax in axes:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djJYdw91oYcJ",
        "colab_type": "text"
      },
      "source": [
        "Let's try some stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnJNJRwNnm2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Showing LIME evidence for faced class')\n",
        "explanation_image, explanation_mask = explanation.get_image_and_mask(0, positive_only=True, num_features=5, hide_rest=True)\n",
        "show_lime_explanation(test_image, explanation_image, explanation_mask)\n",
        "\n",
        "print('Showing LIME evidence for defaced class')\n",
        "explanation_image, explanation_mask = explanation.get_image_and_mask(1, positive_only=True, num_features=5, hide_rest=True)\n",
        "show_lime_explanation(test_image, explanation_image, explanation_mask)\n",
        "\n",
        "print('Showing LIME evidence for faced class, including negative evidence')\n",
        "explanation_image, explanation_mask = explanation.get_image_and_mask(0, positive_only=False, num_features=5, hide_rest=False)\n",
        "show_lime_explanation(test_image, explanation_image, explanation_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWCk2dbe-MRl",
        "colab_type": "text"
      },
      "source": [
        "LIME's `ImageExplainer` class uses \"super pixels\", which groups image regions into a small number of regions. This implementation uses some combination of distance (regions have to be connected) and pixel intensity. How many regions are there? How similar do pixels have to be? You could probably play with these parameters until you get a nice looking explanation.\n",
        "\n",
        "![This is fine](https://amnesia.cbrain.mcgill.ca/deeplearning/img/cant-be-wrong.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GovGURZoVyJq",
        "colab_type": "text"
      },
      "source": [
        "## Layerwise Relevance Propagation\n",
        "\n",
        "LRP is probably the most rigourously mathematically supported linear approximation for importance, but also the most difficult to understand fully. There are many choices of relevant propagation rule, and choices must be made for different types of non-linearity for how to propagate each non-linear region of the function. The authors even suggest different propagation rules for different non-linearities.\n",
        "\n",
        "iNNvestigate GitHub repository that implements it and other interpretation methods for Keras models: [https://github.com/albermax/innvestigate](https://github.com/albermax/innvestigate)\n",
        "\n",
        "LRP Paper: [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140)\n",
        "\n",
        "CVPR Tutorial Video: [https://www.youtube.com/watch?v=LtbM2phNI7I](https://www.youtube.com/watch?v=LtbM2phNI7I)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0cc68NPVzyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/albermax/innvestigate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya9lWPrnV1kS",
        "colab_type": "text"
      },
      "source": [
        "![This is fine](https://amnesia.cbrain.mcgill.ca/deeplearning/img/grinds-gears.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLC67fsVV3Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import innvestigate\n",
        "import innvestigate.utils as iutils\n",
        "import innvestigate.utils.visualizations as ivis\n",
        "\n",
        "from keras import activations\n",
        "\n",
        "if my_model.layers[-1].activation != activations.linear:\n",
        "  model_wo_softmax = iutils.keras.graph.model_wo_softmax(my_model)\n",
        "else:\n",
        "  model_wo_softmax = my_model\n",
        "\n",
        "lrp_analyzer = innvestigate.create_analyzer(\"lrp.z\", model_wo_softmax)\n",
        "\n",
        "image_batch = image_data[0:1, ...]\n",
        "\n",
        "analysis = lrp_analyzer.analyze(image_batch)\n",
        "heatmap = ivis.heatmap(analysis)\n",
        "\n",
        "\n",
        "f, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(image_data[0])\n",
        "axes[1].imshow(heatmap[0])\n",
        "axes[0].axis('off')\n",
        "axes[1].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pIB2mknsIPK",
        "colab_type": "text"
      },
      "source": [
        "The end.\n",
        "\n",
        "![done](https://amnesia.cbrain.mcgill.ca/deeplearning/img/done.png)"
      ]
    }
  ]
}